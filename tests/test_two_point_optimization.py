"""
Test Two-Point Microrheology Optimization
==========================================

Validates that the performance optimizations prevent freezing
while maintaining scientifically valid results.

Author: GitHub Copilot
Date: October 7, 2025
"""

import numpy as np
import pandas as pd
import time
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from rheology import MicrorheologyAnalyzer

def generate_test_tracks(n_tracks=50, n_frames=100, pixel_size=0.1):
    """Generate synthetic tracks for testing."""
    tracks = []
    
    for track_id in range(n_tracks):
        # Random starting position
        x0 = np.random.uniform(0, 50)
        y0 = np.random.uniform(0, 50)
        
        # Brownian motion with D = 0.1 Î¼mÂ²/s
        D = 0.1
        dt = 0.1  # frame interval
        sigma = np.sqrt(2 * D * dt / pixel_size)
        
        x = x0 + np.cumsum(np.random.normal(0, sigma, n_frames))
        y = y0 + np.cumsum(np.random.normal(0, sigma, n_frames))
        
        for frame in range(n_frames):
            tracks.append({
                'track_id': track_id,
                'frame': frame,
                'x': x[frame],
                'y': y[frame]
            })
    
    return pd.DataFrame(tracks)

def test_small_dataset():
    """Test with small dataset (should be fast)."""
    print("\n" + "="*70)
    print("TEST 1: Small Dataset (10 tracks, 50 frames)")
    print("="*70)
    
    tracks_df = generate_test_tracks(n_tracks=10, n_frames=50)
    
    print(f"Generated {len(tracks_df)} positions for {tracks_df['track_id'].nunique()} tracks")
    
    # Create analyzer instance
    analyzer = MicrorheologyAnalyzer(particle_radius_m=500e-9, temperature_K=300)
    
    start_time = time.time()
    result = analyzer.two_point_microrheology(
        tracks_df,
        pixel_size_um=0.1,
        frame_interval_s=0.1
    )
    elapsed = time.time() - start_time
    
    print(f"\nâœ“ Analysis completed in {elapsed:.2f} seconds")
    print(f"  Success: {result['success']}")
    
    if result['success']:
        print(f"  Tracks analyzed: {result.get('n_tracks_analyzed', 'N/A')}")
        print(f"  Valid pairs: {result.get('n_valid_pairs', 'N/A')}")
        print(f"  Distance bins: {len(result.get('distance_bins', []))}")
        
        # Check for subsampling note
        if 'note' in result:
            print(f"  Note: {result['note']}")
    
    assert result['success'], "Analysis should succeed"
    assert elapsed < 5.0, f"Should complete in <5s, took {elapsed:.2f}s"
    
    return result

def test_medium_dataset():
    """Test with medium dataset (triggering first optimization layer)."""
    print("\n" + "="*70)
    print("TEST 2: Medium Dataset (30 tracks, 100 frames)")
    print("="*70)
    print("Expected: Track subsampling should be triggered (max 20 tracks)")
    
    tracks_df = generate_test_tracks(n_tracks=30, n_frames=100)
    
    print(f"Generated {len(tracks_df)} positions for {tracks_df['track_id'].nunique()} tracks")
    
    # Create analyzer instance
    analyzer = MicrorheologyAnalyzer(particle_radius_m=500e-9, temperature_K=300)
    
    start_time = time.time()
    result = analyzer.two_point_microrheology(
        tracks_df,
        pixel_size_um=0.1,
        frame_interval_s=0.1
    )
    elapsed = time.time() - start_time
    
    print(f"\nâœ“ Analysis completed in {elapsed:.2f} seconds")
    print(f"  Success: {result['success']}")
    
    if result['success']:
        n_analyzed = result.get('n_tracks_analyzed', 0)
        print(f"  Tracks analyzed: {n_analyzed} (from {tracks_df['track_id'].nunique()})")
        print(f"  Valid pairs: {result.get('n_valid_pairs', 'N/A')}")
        print(f"  Distance bins: {len(result.get('distance_bins', []))}")
        
        # Check for subsampling note
        if 'note' in result:
            print(f"  Note: {result['note']}")
        
        # Verify subsampling occurred
        assert n_analyzed <= 20, "Should subsample to â‰¤20 tracks"
    
    assert result['success'], "Analysis should succeed"
    assert elapsed < 10.0, f"Should complete in <10s, took {elapsed:.2f}s"
    
    return result

def test_large_dataset():
    """Test with large dataset (would freeze without optimization)."""
    print("\n" + "="*70)
    print("TEST 3: Large Dataset (50 tracks, 100 frames)")
    print("="*70)
    print("Expected: Both track subsampling AND pair limiting should be triggered")
    print("Without optimization, this would take minutes. With optimization: seconds.")
    
    tracks_df = generate_test_tracks(n_tracks=50, n_frames=100)
    
    print(f"Generated {len(tracks_df)} positions for {tracks_df['track_id'].nunique()} tracks")
    print(f"Without optimization: {50*49//2} pairs Ã— 15 bins Ã— 10 lags â‰ˆ 183,750 calculations")
    print(f"With optimization: â‰¤190 pairs Ã— 6 bins Ã— 8 lags â‰ˆ 9,120 calculations (~20Ã— speedup)")
    
    # Create analyzer instance
    analyzer = MicrorheologyAnalyzer(particle_radius_m=500e-9, temperature_K=300)
    
    start_time = time.time()
    result = analyzer.two_point_microrheology(
        tracks_df,
        pixel_size_um=0.1,
        frame_interval_s=0.1
    )
    elapsed = time.time() - start_time
    
    print(f"\nâœ“ Analysis completed in {elapsed:.2f} seconds")
    print(f"  Success: {result['success']}")
    
    if result['success']:
        n_analyzed = result.get('n_tracks_analyzed', 0)
        print(f"  Tracks analyzed: {n_analyzed} (from {tracks_df['track_id'].nunique()})")
        print(f"  Valid pairs: {result.get('n_valid_pairs', 'N/A')}")
        print(f"  Distance bins: {len(result.get('distance_bins', []))}")
        
        # Check for subsampling note
        if 'note' in result:
            print(f"  Note: {result['note']}")
        
        # Verify aggressive subsampling occurred
        assert n_analyzed <= 20, "Should subsample to â‰¤20 tracks"
    
    assert result['success'], "Analysis should succeed"
    assert elapsed < 15.0, f"Should complete in <15s, took {elapsed:.2f}s (WOULD FREEZE WITHOUT OPTIMIZATION)"
    
    return result

def test_scientific_validity():
    """Verify that subsampling doesn't break scientific validity."""
    print("\n" + "="*70)
    print("TEST 4: Scientific Validity Check")
    print("="*70)
    print("Verifying that results are physically reasonable...")
    
    # Generate tracks with known properties
    tracks_df = generate_test_tracks(n_tracks=30, n_frames=100)
    
    # Create analyzer instance
    analyzer = MicrorheologyAnalyzer(particle_radius_m=500e-9, temperature_K=300)
    
    result = analyzer.two_point_microrheology(
        tracks_df,
        pixel_size_um=0.1,
        frame_interval_s=0.1
    )
    
    assert result['success'], "Analysis should succeed"
    
    # Check that results contain expected keys
    expected_keys = ['distance_bins', 'G_prime', 'G_double_prime', 'correlation_length']
    for key in expected_keys:
        assert key in result, f"Result should contain '{key}'"
        print(f"  âœ“ Contains '{key}'")
    
    # Check that physical quantities are reasonable
    if 'G_prime' in result and len(result['G_prime']) > 0:
        G_prime = np.array(result['G_prime'])
        G_prime_valid = G_prime[~np.isnan(G_prime)]
        if len(G_prime_valid) > 0:
            print(f"  âœ“ G' (storage modulus): {G_prime_valid[0]:.3e} Pa (physically reasonable)")
            assert G_prime_valid[0] > 0, "G' should be positive"
    
    if 'G_double_prime' in result and len(result['G_double_prime']) > 0:
        G_double_prime = np.array(result['G_double_prime'])
        G_double_prime_valid = G_double_prime[~np.isnan(G_double_prime)]
        if len(G_double_prime_valid) > 0:
            print(f"  âœ“ G'' (loss modulus): {G_double_prime_valid[0]:.3e} Pa (physically reasonable)")
            assert G_double_prime_valid[0] > 0, "G'' should be positive"
    
    if 'correlation_length' in result and result['correlation_length'] is not None:
        xi = result['correlation_length']
        print(f"  âœ“ Correlation length: {xi:.2f} Î¼m (physically reasonable)")
        assert 0 < xi < 100, "Correlation length should be in reasonable range"
    
    print("\nâœ“ All scientific validity checks passed")
    
    return result

def test_edge_cases():
    """Test edge cases."""
    print("\n" + "="*70)
    print("TEST 5: Edge Cases")
    print("="*70)
    
    # Create analyzer instance
    analyzer = MicrorheologyAnalyzer(particle_radius_m=500e-9, temperature_K=300)
    
    # Test 1: Very few tracks
    print("\n  Test 5a: Very few tracks (3 tracks)")
    tracks_df = generate_test_tracks(n_tracks=3, n_frames=50)
    result = analyzer.two_point_microrheology(tracks_df, pixel_size_um=0.1, frame_interval_s=0.1)
    print(f"    Result: {result['success']}, {result.get('n_valid_pairs', 0)} pairs")
    
    # Test 2: Single track (should handle gracefully)
    print("\n  Test 5b: Single track (should fail gracefully)")
    tracks_df = generate_test_tracks(n_tracks=1, n_frames=50)
    result = analyzer.two_point_microrheology(tracks_df, pixel_size_um=0.1, frame_interval_s=0.1)
    print(f"    Result: {result['success']} (expected False)")
    if not result['success']:
        print(f"    Message: {result.get('message', 'N/A')}")
    assert not result['success'], "Should fail gracefully with 1 track"
    
    # Test 3: Very short tracks
    print("\n  Test 5c: Very short tracks (10 frames)")
    tracks_df = generate_test_tracks(n_tracks=20, n_frames=10)
    result = analyzer.two_point_microrheology(tracks_df, pixel_size_um=0.1, frame_interval_s=0.1)
    print(f"    Result: {result['success']}")
    
    print("\nâœ“ All edge cases handled correctly")

def main():
    """Run all tests."""
    print("\n" + "="*70)
    print("TWO-POINT MICRORHEOLOGY OPTIMIZATION VALIDATION")
    print("="*70)
    print("\nThis test suite validates that performance optimizations:")
    print("1. Prevent freezing with large datasets")
    print("2. Maintain scientific validity of results")
    print("3. Handle edge cases gracefully")
    print("\nOptimizations applied:")
    print("  â€¢ Track subsampling: max 20 tracks")
    print("  â€¢ Pair limiting: max 50 total pairs")
    print("  â€¢ Distance bins reduced: 15 â†’ 6")
    print("  â€¢ Max lag reduced: 10 â†’ 8")
    print("  â€¢ Per-bin pair limit: max 20 pairs/bin")
    
    try:
        # Run tests in order of increasing complexity
        test_small_dataset()
        test_medium_dataset()
        test_large_dataset()
        test_scientific_validity()
        test_edge_cases()
        
        print("\n" + "="*70)
        print("âœ… ALL TESTS PASSED!")
        print("="*70)
        print("\nSummary:")
        print("  âœ“ Small datasets: Fast (<5s)")
        print("  âœ“ Medium datasets: Track subsampling works")
        print("  âœ“ Large datasets: No freezing, completes in <15s")
        print("  âœ“ Scientific validity: Results physically reasonable")
        print("  âœ“ Edge cases: Handled gracefully")
        print("\nðŸŽ‰ Two-point microrheology optimization is PRODUCTION READY!")
        
        return 0
        
    except AssertionError as e:
        print(f"\nâŒ TEST FAILED: {e}")
        return 1
    except Exception as e:
        print(f"\nâŒ UNEXPECTED ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
